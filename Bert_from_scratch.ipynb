{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdpxHowLeK7L"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade keras-nlp tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries"
      ],
      "metadata": {
        "id": "nC5wxqRufMBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import keras_nlp\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "policy=keras.mixed_precision.Policy(\"mixed_float16\")\n",
        "keras.mixed_precision.set_global_policy(policy)"
      ],
      "metadata": {
        "id": "hHrm8n6Aes9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get Dataset"
      ],
      "metadata": {
        "id": "uDHs_aONfRM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q8IqtVf7fRC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download pretrainning data\n",
        "link_to_dataset=\"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\"\n",
        "\n",
        "keras.utils.get_file(origin=link_to_dataset,\n",
        "                     extract=True)\n",
        "wiki_dir=os.path.expanduser(\"~/.keras/datasets/wikitext-103-raw/\")\n",
        "#download finetuning data\n",
        "link_to_fine_tune_dataset=\"https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\"\n",
        "keras.utils.get_file(origin=link_to_fine_tune_dataset,\n",
        "                     extract=True)\n",
        "\n",
        "sst_dir=os.path.expanduser(\"~/.keras/datasets/SST-2/\")\n",
        "\n",
        "#download vocabulary data\n",
        "vcab_text_link=\"https://storage.googleapis.com/tensorflow/keras-nlp/examples/bert/bert_vocab_uncased.txt\"\n",
        "vacab_file=keras.utils.get_file(\n",
        "    origin=vcab_text_link\n",
        ")"
      ],
      "metadata": {
        "id": "ZzLXSPH2fUHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing params\n",
        "PRETRAINING_BATCH_SIZE=128\n",
        "FINETUNING_BATCH_SIZE=32\n",
        "SEQ_LENGTH=128\n",
        "MASK_RATE=0.25\n",
        "PREDICTIONS_PER_SEQ=32"
      ],
      "metadata": {
        "id": "8v1KEBdkjhlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load sst2 data set\n",
        "sst_train_ds=tf.data.experimental.CsvDataset(sst_dir+\"train.tsv\",[tf.string,tf.int32],header=True,field_delim=\"\\t\").batch(FINETUNING_BATCH_SIZE)\n",
        "sst_val_ds=tf.data.experimental.CsvDataset(sst_dir+\"dev.tsv\",[tf.string,tf.int32],header=True,field_delim=\"\\t\").batch(FINETUNING_BATCH_SIZE)\n",
        "\n",
        "#load wiki text dataset and filter short lines\n",
        "wiki_train_ds=(\n",
        "    tf.data.TextLineDataset(wiki_dir+\"wiki.valid.raw\")\n",
        "    .filter(lambda x: tf.strings.length(x)>100)\n",
        "    .batch(PRETRAINING_BATCH_SIZE)\n",
        ")\n",
        "\n",
        "wiki_valid_dataset=(\n",
        "    tf.data.TextLineDataset(wiki_dir+\"wiki.valid.raw\")\n",
        "    .filter(lambda x: tf.strings.length(x)>100)\n",
        "    .batch(PRETRAINING_BATCH_SIZE)\n",
        ")\n",
        "\n",
        "#take a peek at sst-2 dataset\n",
        "print(sst_train_ds.unbatch().batch(4).take(1).get_single_element())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZBgZU2Pj1-q",
        "outputId": "1c1e963b-630b-401b-de29-e144bdf60d47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
            "array([b'hide new secretions from the parental units ',\n",
            "       b'contains no wit , only labored gags ',\n",
            "       b'that loves its characters and communicates something rather beautiful about human nature ',\n",
            "       b'remains utterly satisfied to remain the same throughout '],\n",
            "      dtype=object)>, <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 1, 0], dtype=int32)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Two main components\n",
        "\n",
        "keras_nlp.tokenizers.Tokenizer- transform text into sequence of input tekoen_ids\n",
        "\n",
        "keras_nlp.tokenizers.WordPieceTokenizer-subword tokenization , its pular when training on a large corpa\n",
        "it allwos model to learn uncommon words\n",
        "while not requiring massive vacab of every word in our training set\n",
        "\n",
        "keras_nlp.layers.MaskedLMMaskGenerator\n",
        "this randomly selects a set of imput and mask them out\n",
        "\n",
        "tf.data.Dataset.map -both tokenizer and masking can be used in a call\n",
        "\n",
        "tf.data- efficiently precompute each batch on cpu"
      ],
      "metadata": {
        "id": "uMp7S-Gsmggw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#example\n",
        "vocab=[\"[UNK]\",\"the\",\"qu\",\"##ick\",\"br\",\"##own\",\"fox\",\".\"]\n",
        "inputs=[\"The quick brown fox.\"]\n",
        "\n",
        "#implimentation of word piece tokenizer\n",
        "tokenizer=keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    lowercase=True\n",
        ")\n",
        "\n",
        "tokenizer(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOWMjudSmMys",
        "outputId": "2bfc330f-a9d4-48e4-fbdc-34100bddb933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[1, 2, 3, 4, 5, 6, 7]]>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#example for string op\n",
        "vocab=[\"[UNK]\",\"the\",\"qu\",\"##ick\",\"br\",\"##own\",\"fox\",\".\"]\n",
        "inputs=[\"The quick brown fox.\"]\n",
        "\n",
        "#implimentation of word piece tokenizer\n",
        "tokenizer=keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    lowercase=True,\n",
        "    dtype=\"string\",\n",
        ")\n",
        "\n",
        "tokenizer(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7hPU-hBpK4m",
        "outputId": "f4fb4491-bbe6-424e-82fb-e561afa59531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'the', b'qu', b'##ick', b'br', b'##own', b'fox', b'.']]>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing params\n",
        "PRETRAINING_BATCH_SIZE=128\n",
        "FINETUNING_BATCH_SIZE=32\n",
        "SEQ_LENGTH=128\n",
        "MASK_RATE=0.25\n",
        "PREDICTIONS_PER_SEQ=32\n",
        "\n",
        "#model params\n",
        "#encoder stack\n",
        "NUM_LAYERS=3 #12 or 24 in general\n",
        "MODEL_DIM=256 #512 min\n",
        "INTERMIDIATE_DIM=512\n",
        "NUM_HEAD=4 #change number\n",
        "DROPOUT=0.1\n",
        "NORM_EPSILON=1e-5\n",
        "\n",
        "#training params\n",
        "PRETRAINING_LEARNING_RATE=5e-4\n",
        "PRETRAINING_EPOCHS=8\n",
        "FINETUNEING_LEANING_RATE=5e-5\n",
        "FINETUNING_EPOCHS=3\n"
      ],
      "metadata": {
        "id": "-apqp9a9pl2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting sequence length will trim or pad the token ops to shape\n",
        "# (batchsize,seqlength)\n",
        "tokenizer=keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vacab_file,\n",
        "    sequence_length=SEQ_LENGTH,\n",
        "    lowercase=True,\n",
        "    strip_accents=True\n",
        ")\n",
        "\n",
        "#setting mask_selection_length will trim or pad the token ops to shape\n",
        "# (batchsize,PREDICTIONS_PER_SEQ)\n",
        "\n",
        "masker=keras_nlp.layers.MaskedLMMaskGenerator(\n",
        "    vocabulary_size=tokenizer.vocabulary_size(),\n",
        "    mask_selection_rate=MASK_RATE,\n",
        "    mask_selection_length=PREDICTIONS_PER_SEQ,\n",
        "    mask_token_id=tokenizer.token_to_id(\"[MASK]\")\n",
        "\n",
        ")\n",
        "\n",
        "def preprocess(inputs):\n",
        "  inputs=tokenizer(inputs)\n",
        "  outputs=masker(inputs)\n",
        "\n",
        "  #split the masking layer ops into a (feature ,labels and weights)\n",
        "  #tuple that so that we can use keras.Model.fit()\n",
        "\n",
        "  features={\n",
        "      \"token_ids\":outputs[\"token_ids\"],\n",
        "      \"mask_positions\":outputs[\"mask_positions\"]\n",
        "  }\n",
        "\n",
        "  labels=outputs[\"mask_ids\"]\n",
        "  weights=outputs[\"mask_weights\"]\n",
        "\n",
        "  return features,labels,weights\n",
        "\n",
        "\n",
        "#we use prefetch to precompute preprocessed batched on the fly on cpu\n",
        "\n",
        "pretrain_ds=wiki_train_ds.map(\n",
        "      preprocess,num_parallel_calls=tf.data.AUTOTUNE\n",
        "  ).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "pretrain_val_ds=wiki_valid_dataset.map(\n",
        "      preprocess,num_parallel_calls=tf.data.AUTOTUNE\n",
        "  ).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#preview a single input example\n",
        "\n",
        "#the mask will change each time you run the cell\n",
        "print(pretrain_val_ds.take(1).get_single_element())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMMegf83qj3f",
        "outputId": "6ac1de27-742a-44f9-ee9e-06104857bc6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'token_ids': <tf.Tensor: shape=(128, 128), dtype=int32, numpy=\n",
            "array([[7570, 7849, 2271, ..., 9673, 1012,  103],\n",
            "       [7570,  103, 2271, ..., 1007, 1012, 2023],\n",
            "       [1996, 2034, 3940, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [2076, 1996, 2307, ...,    0,    0,    0],\n",
            "       [3216, 2225, 2083, ...,    0,    0,    0],\n",
            "       [9794, 2007,  103, ...,    0,    0,    0]], dtype=int32)>, 'mask_positions': <tf.Tensor: shape=(128, 32), dtype=int64, numpy=\n",
            "array([[  5,  11,  15, ..., 112, 119, 127],\n",
            "       [  1,   5,   6, ..., 121, 122, 124],\n",
            "       [  3,   8,  11, ...,   0,   0,   0],\n",
            "       ...,\n",
            "       [  9,  11,  18, ..., 119, 120,   0],\n",
            "       [ 15,  21,  22, ...,   0,   0,   0],\n",
            "       [  2,   4,  17, ...,   0,   0,   0]])>}, <tf.Tensor: shape=(128, 32), dtype=int32, numpy=\n",
            "array([[ 1010,  2030,  2003, ...,  1996,  2077,  7570],\n",
            "       [ 7849,  2003,  1037, ...,  1006,  9587,  2075],\n",
            "       [ 1997,  4273,  2312, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [23133,  1996,  6032, ...,  3462,  1012,     0],\n",
            "       [ 2264,  1010,  6041, ...,     0,     0,     0],\n",
            "       [ 1045,  1011,  8820, ...,     0,     0,     0]], dtype=int32)>, <tf.Tensor: shape=(128, 32), dtype=float16, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 0.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.]], dtype=float16)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#building block of bert which is an encoder only block\n",
        "#transformer encoder layer\n",
        "\n",
        "# This class follows the architecture of the transformer encoder layer in the\n",
        "# paper Attention is All You Need. Users\n",
        "# can instantiate multiple instances of this class to stack up an encoder.\n",
        "\n",
        "# This layer will correctly compute an attention mask from an implicit\n",
        "# Keras padding mask (for example, by passing mask_zero=True to a\n",
        "# keras.layers.Embedding layer).\n",
        "\n",
        "# keras_nlp.layers.TransformerEncoder(\n",
        "#     intermediate_dim,\n",
        "#     num_heads,\n",
        "#     dropout=0,\n",
        "#     activation=\"relu\",\n",
        "#     layer_norm_epsilon=1e-5,\n",
        "#     kernel_initializer='golrot_uniform',\n",
        "#     bias_initializer=\"zeros\",\n",
        "#     name=None,\n",
        "#     **kwargs\n",
        "# )\n",
        "\n",
        "\n",
        "#simple example of Token and positioning embedding\n",
        "\n",
        "# keras.nlp.layers.TokenANDPositionEmbedding(\n",
        "#     vacabulary_size,\n",
        "#     sequence_length,\n",
        "#     embedding_dim,\n",
        "#     embedding_initializer=\"glorot_uniform\",\n",
        "#     mask_zero=False**kwargs\n",
        "\n",
        "# )\n",
        "\n",
        "# Layer normalization layer (Ba et al., 2016).\n",
        "\n",
        "# Normalize the activations of the previous layer for each given example in a\n",
        "# batch independently, rather than across a batch like Batch Normalization.\n",
        "# i.e. applies a transformation that maintains the mean activation within each\n",
        "# example close to 0 and the activation standard deviation close to 1.\n",
        "\n",
        "# Given a tensor inputs, moments are calculated and normalization\n",
        "# is performed across the axes specified in axis.\n",
        "\n",
        "\n",
        "# tf.keras.layers.LayerNormalization(\n",
        "#     axis=-1,\n",
        "#     epsilon=0.001,\n",
        "#     centre=True,\n",
        "#     scle=True,\n",
        "#     beta_initializer=\"zeros\",\n",
        "#     gamma_initilizer=\"ones\",\n",
        "#     beta_regularizer=None,\n",
        "#     gamma_regularizer=None,\n",
        "#     beta_constraint=None,\n",
        "#     gamma_constraint=None,\n",
        "#     **kwargs\n",
        "# )\n",
        "\n",
        "# tf.keras.layers.Dropout(rate,noise_shape=None,seed=None,**kwargs)\n"
      ],
      "metadata": {
        "id": "nRtFpod0wg8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pretraining the model\n"
      ],
      "metadata": {
        "id": "pk5qDfMSczPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input layer\n",
        "inputs=keras.Input(shape=(SEQ_LENGTH,),dtype=tf.int32)\n",
        "\n",
        "#embedding layer for token and position\n",
        "embedding_layer=keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=tokenizer.vocabulary_size(),\n",
        "    sequence_length=SEQ_LENGTH,\n",
        "    embedding_dim=MODEL_DIM\n",
        ")\n",
        "\n",
        "#op of embedding layer\n",
        "outputs=embedding_layer(inputs)\n",
        "#add a layer normalisation\n",
        "outputs=keras.layers.LayerNormalization(epsilon=NORM_EPSILON)(outputs)\n",
        "#dd a dropout\n",
        "outputs=keras.layers.Dropout(rate=DROPOUT)(outputs)\n",
        "\n",
        "#added the attention mechanism / encoder layer of keras\n",
        "for i in range(NUM_LAYERS):\n",
        "  outputs=keras_nlp.layers.TransformerEncoder(\n",
        "      intermediate_dim=INTERMIDIATE_DIM,\n",
        "      num_heads=NUM_HEAD,\n",
        "      dropout=DROPOUT,\n",
        "      layer_norm_epsilon=NORM_EPSILON\n",
        "  )(outputs)\n",
        "\n",
        "#create the model having ip and op\n",
        "encoder_model= keras.Model(inputs,outputs)\n",
        "\n",
        "#print summary of the model\n",
        "encoder_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FeHKIbBCIIo",
        "outputId": "dbaac94e-48b6-4c26-dd35-595397205141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_16 (InputLayer)       [(None, 128)]             0         \n",
            "                                                                 \n",
            " token_and_position_embeddi  (None, 128, 256)          7846400   \n",
            " ng_1 (TokenAndPositionEmbe                                      \n",
            " dding)                                                          \n",
            "                                                                 \n",
            " layer_normalization_1 (Lay  (None, 128, 256)          512       \n",
            " erNormalization)                                                \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128, 256)          0         \n",
            "                                                                 \n",
            " transformer_encoder_3 (Tra  (None, 128, 256)          527104    \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " transformer_encoder_4 (Tra  (None, 128, 256)          527104    \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " transformer_encoder_5 (Tra  (None, 128, 256)          527104    \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9428224 (35.97 MB)\n",
            "Trainable params: 9428224 (35.97 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now pretrain the transformer model\n",
        "#create a pretrain model by attachig a mask language model head\n",
        "inputs={\n",
        "    \"token_ids\":keras.Input(shape=(SEQ_LENGTH,),dtype=tf.int32),\n",
        "    \"mask_positions\":keras.Input(shape=(PREDICTIONS_PER_SEQ,),dtype=tf.int32)\n",
        "}\n",
        "\n",
        "encoded_tokens=encoder_model(inputs[\"token_ids\"])\n",
        "\n",
        "# predict output word for each masked token_id\n",
        "# token embeddings to project from our encoded vectors to\n",
        "# vocabulary logits which has been shown to improve training efficiency\n",
        "\n",
        "#bug in mask_positions used masked_positions\n",
        "#github issue https://github.com/keras-team/keras-io/issues/1446\n",
        "outputs=keras_nlp.layers.MaskedLMHead(\n",
        "    embedding_weights=embedding_layer.token_embedding.embeddings,\n",
        "    activation=\"softmax\",\n",
        "    )(encoded_tokens,inputs[\"mask_positions\"])\n",
        "\n",
        "\n",
        "#define the model\n",
        "pretraining_model=keras.Model(inputs,outputs)\n",
        "\n",
        "#compile the model\n",
        "pretraining_model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.experimental.AdamW(PRETRAINING_LEARNING_RATE),\n",
        "    weighted_metrics=[\"sparse_categorical_accuracy\"],\n",
        "    jit_compile=True\n",
        ")\n",
        "\n",
        "#pretrain model on wiki text\n",
        "pretraining_model.fit(\n",
        "    pretrain_ds,\n",
        "    validation_data=pretrain_val_ds,\n",
        "    epochs=PRETRAINING_EPOCHS\n",
        ")\n",
        "\n",
        "encoder_model.save(\"encoder_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlGxTdooInPH",
        "outputId": "60464d26-691c-4ee0-cddb-e93425cfd31a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "13/13 [==============================] - 61s 2s/step - loss: 7.9326 - sparse_categorical_accuracy: 0.0395 - val_loss: 7.1872 - val_sparse_categorical_accuracy: 0.0627\n",
            "Epoch 2/8\n",
            "13/13 [==============================] - 4s 326ms/step - loss: 6.7316 - sparse_categorical_accuracy: 0.0464 - val_loss: 6.2698 - val_sparse_categorical_accuracy: 0.0464\n",
            "Epoch 3/8\n",
            "13/13 [==============================] - 3s 223ms/step - loss: 6.0677 - sparse_categorical_accuracy: 0.0514 - val_loss: 5.8673 - val_sparse_categorical_accuracy: 0.0471\n",
            "Epoch 4/8\n",
            "13/13 [==============================] - 3s 206ms/step - loss: 5.8100 - sparse_categorical_accuracy: 0.0634 - val_loss: 5.7967 - val_sparse_categorical_accuracy: 0.0646\n",
            "Epoch 5/8\n",
            "13/13 [==============================] - 3s 206ms/step - loss: 5.8031 - sparse_categorical_accuracy: 0.0641 - val_loss: 5.7944 - val_sparse_categorical_accuracy: 0.0632\n",
            "Epoch 6/8\n",
            "13/13 [==============================] - 3s 271ms/step - loss: 5.7853 - sparse_categorical_accuracy: 0.0650 - val_loss: 5.7854 - val_sparse_categorical_accuracy: 0.0647\n",
            "Epoch 7/8\n",
            "13/13 [==============================] - 3s 210ms/step - loss: 5.7911 - sparse_categorical_accuracy: 0.0634 - val_loss: 5.7743 - val_sparse_categorical_accuracy: 0.0637\n",
            "Epoch 8/8\n",
            "13/13 [==============================] - 3s 209ms/step - loss: 5.7671 - sparse_categorical_accuracy: 0.0638 - val_loss: 5.7872 - val_sparse_categorical_accuracy: 0.0636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[\"mask_positions\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hntqg2FGQfVY",
        "outputId": "d72dcc53-c048-4eaa-9f9a-ec68f09b7a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 32) dtype=int32 (created by layer 'input_26')>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine Tuning"
      ],
      "metadata": {
        "id": "G7mSwOSuMUmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sentences,labels):\n",
        "  return tokenizer(sentences),labels\n",
        "\n",
        "finetune_ds=sst_train_ds.map(\n",
        "    preprocess,num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "finetune_val_ds=sst_val_ds.map(\n",
        "    preprocess,num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "print(finetune_val_ds.take(1).get_single_element())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCGct26WfPLJ",
        "outputId": "004c1dae-95c3-4add-9f77-b4b73cc5ba3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(32, 128), dtype=int32, numpy=\n",
            "array([[ 2009,  1005,  1055, ...,     0,     0,     0],\n",
            "       [ 4895, 10258,  2378, ...,     0,     0,     0],\n",
            "       [ 4473,  2149,  2000, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [ 1045,  2018,  2000, ...,     0,     0,     0],\n",
            "       [ 4283,  2000,  3660, ...,     0,     0,     0],\n",
            "       [ 1012,  1012,  1012, ...,     0,     0,     0]], dtype=int32)>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
            "array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
            "       0, 1, 1, 0, 0, 1, 0, 0, 1, 0], dtype=int32)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model=keras.models.load_model(\"encoder_model\",compile=False)\n",
        "\n",
        "#tokenized input\n",
        "inputs=keras.Input(shape=(SEQ_LENGTH,),dtype=tf.int32)\n",
        "\n",
        "#encode and pool the tokens\n",
        "encoded_tokens=encoder_model(inputs)\n",
        "\n",
        "pooled_tokens=keras.layers.GlobalAveragePooling1D()(encoded_tokens)\n",
        "\n",
        "#predict output label\n",
        "outputs=keras.layers.Dense(1,activation=\"sigmoid\")(pooled_tokens)\n",
        "\n",
        "#define and compile model\n",
        "finetune_model=keras.Model(inputs,outputs)\n",
        "finetune_model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=keras.optimizers.experimental.AdamW(FINETUNEING_LEANING_RATE),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "#finetune the model for SST-2 Task\n",
        "finetune_model.fit(\n",
        "    finetune_ds,\n",
        "    validation_data=finetune_val_ds,\n",
        "    epochs=FINETUNING_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqrMawcsfPHf",
        "outputId": "e71a1926-be2a-4733-f265-745655823227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "2105/2105 [==============================] - 123s 51ms/step - loss: 0.5600 - accuracy: 0.6702 - val_loss: 0.4428 - val_accuracy: 0.8062\n",
            "Epoch 2/3\n",
            "2105/2105 [==============================] - 69s 33ms/step - loss: 0.2693 - accuracy: 0.8909 - val_loss: 0.4599 - val_accuracy: 0.8073\n",
            "Epoch 3/3\n",
            "2105/2105 [==============================] - 69s 33ms/step - loss: 0.2140 - accuracy: 0.9159 - val_loss: 0.5071 - val_accuracy: 0.7959\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e294bd45ab0>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the model along with the tokenization layer.\n",
        "# as with the keras_nlp preprocessing is done inside tensorflow graph\n",
        "#benifit is the model can be saved and restoref , that can directly run inefrence on raw text\n",
        "#you dont have to load tokenizer seperately\n",
        "\n",
        "\n",
        "#so add tokenization into final model\n",
        "inputs=keras.Input(shape=(),dtype=tf.string)\n",
        "tokens=tokenizer(inputs)\n",
        "outputs=finetune_model(tokens)\n",
        "final_model=keras.Model(inputs,outputs)\n",
        "\n",
        "final_model.save(\"final_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8SsQjdifPEt",
        "outputId": "5ecb0996-c9b3-4ed6-f717-a48bc3f7373f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Inference"
      ],
      "metadata": {
        "id": "w4Jcd9wPlVWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#now the model can directly predict raw text\n",
        "\n",
        "\n",
        "#reload model\n",
        "restore_finetuned_model=keras.models.load_model(\"final_model\")\n",
        "\n",
        "#second start inference\n",
        "\n",
        "inference_data=tf.constant([\"terrible,no good,trash\",\"So great: I loved it!\"])\n",
        "print(\"inference on pretrained and finetuned bert model =\",restore_finetuned_model.predict(inference_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkqUsk_5lYsN",
        "outputId": "231c8127-9c54-4ce4-d280-f1ed040ec2d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 995ms/step\n",
            "inference on pretrained and finetuned bert model = [[0.04788]\n",
            " [1.     ]]\n"
          ]
        }
      ]
    }
  ]
}